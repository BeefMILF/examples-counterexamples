{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import bs4\n",
    "\n",
    "import tika\n",
    "from tika import parser\n",
    "\n",
    "from mlutil.parallel import mapp\n",
    "from mlutil.topic_modeling import top_topic_words, topic_coherence\n",
    "import mlutil.parallel as parallel\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.summarization import keywords as textrank_keywords, summarize as textrank_summarize\n",
    "\n",
    "import langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data path\n",
    "\n",
    "Change this to point to your documents folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/kuba/Downloads/książki_nauka'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tika.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_name(path):\n",
    "    return os.path.relpath(path, data_path)\n",
    "\n",
    "\n",
    "\n",
    "def clear_markup(xml_content):\n",
    "    return bs4.BeautifulSoup(xml_content, 'lxml').get_text()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub('\\W+', ' ', re.sub('\\d+', ' ', text).lower())\n",
    "\n",
    "\n",
    "def truncate_pages(xml_content, select_pages=5):\n",
    "    pages = xml_content.split('<div class=\"page\">', max(select_pages))\n",
    "    selected_pages = [pages[i] for i in select_pages]    \n",
    "    return ' '.join(selected_pages)\n",
    "\n",
    "\n",
    "def extract_pdf_text(pdf_path, long_document_threshold=50, short_document_pages=5, long_document_pages=range(5, 10)):\n",
    "    try:\n",
    "        parsed = tika.parser.from_file(pdf_path, xmlContent=True)\n",
    "        num_pages = int(parsed['metadata']['xmpTPg:NPages'])\n",
    "        if num_pages > long_document_threshold:\n",
    "            seleted_content = truncate_pages(parsed['content'], long_document_pages)\n",
    "        else:\n",
    "            seleted_content = truncate_pages(parsed['content'], range(short_document_pages))\n",
    "        return clean_text(clear_markup(seleted_content))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def textrank_summarize_with_fallback(text):\n",
    "    try:\n",
    "        summary = textrank_summarize(text, ratio=0.5)\n",
    "    except:\n",
    "        print('WARNING: article with invalid text')\n",
    "        summary = ''\n",
    "    return summary\n",
    "\n",
    "\n",
    "def is_in_wn(word):\n",
    "    try:\n",
    "        synsets = list(wn.synsets(word))\n",
    "    except:\n",
    "        return False\n",
    "    return len(synsets) > 0\n",
    "\n",
    "\n",
    "def legal_words(text):\n",
    "    for w in text.split():\n",
    "        if is_in_wn(w):\n",
    "            yield w\n",
    "\n",
    "\n",
    "def illegal_vectorizer_words(texts):\n",
    "    # use these as stopwords for actual vectorizer\n",
    "    base_vectorizer = TfidfVectorizer()\n",
    "    base_vectorizer.fit(texts)\n",
    "    return frozenset(w for w in base_vectorizer.vocabulary_.keys() if not is_in_wn(w))\n",
    "\n",
    "\n",
    "def get_most_representative_article_names(topic_index, article_names, topic_features):\n",
    "    topic_scores = topic_features[:,topic_index]\n",
    "    sorted_topic_scores = sorted(enumerate(topic_scores), key=itemgetter(1), reverse=True)\n",
    "    most_representative_articles_indices = list(map(itemgetter(0), sorted_topic_scores))\n",
    "    return [(article_names[i], topic_score) for (i, topic_score) in sorted_topic_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_paths = glob.glob(os.path.join(data_path, '**', '*.pdf'), recursive=True)\n",
    "len(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-01 14:14:13,890 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2019-05-01 14:22:02,390 [MainThread  ] [WARNI]  Tika server returned status: 422\n",
      "2019-05-01 14:22:17,961 [MainThread  ] [WARNI]  Tika server returned status: 422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 797 ms, sys: 468 ms, total: 1.27 s\n",
      "Wall time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = list(mapp(extract_pdf_text, pdf_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [00:08<00:00, 86.55it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "legal_texts_with_filenames = [\n",
    "    (get_article_name(path), text)\n",
    "    for (path, text) in tqdm(zip(pdf_paths, texts), total=len(texts))\n",
    "    if text is not None and len(text) > 100 and langdetect.detect(text[:100]) == 'en'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_texts = [text for (__, text) in legal_texts_with_filenames]\n",
    "legal_filenames = [filename for (filename, __) in legal_texts_with_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text\n",
    "\n",
    "We'll use TF-IDF features (they work best for NMF).\n",
    "Tokens that aren't in WordNet are deleted so that:\n",
    "- we throw out some nonsensical tokens\n",
    "- we keep only words that can be used to evaluate topic coherence using WordNet based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_words = illegal_vectorizer_words(legal_texts)\n",
    "stop_words = illegal_words.union(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "vectorizer = TfidfVectorizer(max_features=2500, stop_words=stop_words)\n",
    "text_vectors = vectorizer.fit_transform(legal_texts)\n",
    "\n",
    "n_samples, n_features = text_vectors.shape\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "\n",
    "We'll use NMF model (Frobenius norm) with tf-idf features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=498 and n_features=2500...\n",
      "CPU times: user 1.93 s, sys: 1.1 s, total: 3.03 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1,cd \n",
    "          alpha=.1, l1_ratio=.5)\n",
    "\n",
    "nmf_features = nmf.fit_transform(text_vectors)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_0</th>\n",
       "      <td>data</td>\n",
       "      <td>matrix</td>\n",
       "      <td>model</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>probability</td>\n",
       "      <td>function</td>\n",
       "      <td>distribution</td>\n",
       "      <td>log</td>\n",
       "      <td>figure</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>let</td>\n",
       "      <td>theorem</td>\n",
       "      <td>proof</td>\n",
       "      <td>set</td>\n",
       "      <td>space</td>\n",
       "      <td>lemma</td>\n",
       "      <td>spaces</td>\n",
       "      <td>finite</td>\n",
       "      <td>definition</td>\n",
       "      <td>proposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>type</td>\n",
       "      <td>list</td>\n",
       "      <td>java</td>\n",
       "      <td>string</td>\n",
       "      <td>code</td>\n",
       "      <td>class</td>\n",
       "      <td>function</td>\n",
       "      <td>chapter</td>\n",
       "      <td>object</td>\n",
       "      <td>program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_3</th>\n",
       "      <td>word</td>\n",
       "      <td>words</td>\n",
       "      <td>language</td>\n",
       "      <td>sentence</td>\n",
       "      <td>document</td>\n",
       "      <td>al</td>\n",
       "      <td>corpus</td>\n",
       "      <td>sentences</td>\n",
       "      <td>text</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_4</th>\n",
       "      <td>learning</td>\n",
       "      <td>training</td>\n",
       "      <td>neural</td>\n",
       "      <td>networks</td>\n",
       "      <td>layer</td>\n",
       "      <td>network</td>\n",
       "      <td>deep</td>\n",
       "      <td>input</td>\n",
       "      <td>hidden</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_5</th>\n",
       "      <td>michael</td>\n",
       "      <td>department</td>\n",
       "      <td>science</td>\n",
       "      <td>computer</td>\n",
       "      <td>image</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>sparse</td>\n",
       "      <td>patch</td>\n",
       "      <td>patches</td>\n",
       "      <td>min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_6</th>\n",
       "      <td>spark</td>\n",
       "      <td>data</td>\n",
       "      <td>apache</td>\n",
       "      <td>cluster</td>\n",
       "      <td>python</td>\n",
       "      <td>model</td>\n",
       "      <td>user</td>\n",
       "      <td>https</td>\n",
       "      <td>import</td>\n",
       "      <td>docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_7</th>\n",
       "      <td>things</td>\n",
       "      <td>world</td>\n",
       "      <td>god</td>\n",
       "      <td>mind</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>life</td>\n",
       "      <td>human</td>\n",
       "      <td>ideas</td>\n",
       "      <td>man</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_8</th>\n",
       "      <td>category</td>\n",
       "      <td>set</td>\n",
       "      <td>categories</td>\n",
       "      <td>object</td>\n",
       "      <td>objects</td>\n",
       "      <td>isomorphism</td>\n",
       "      <td>limits</td>\n",
       "      <td>arrows</td>\n",
       "      <td>arrow</td>\n",
       "      <td>diagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_9</th>\n",
       "      <td>quantum</td>\n",
       "      <td>classical</td>\n",
       "      <td>state</td>\n",
       "      <td>mechanics</td>\n",
       "      <td>learning</td>\n",
       "      <td>doi</td>\n",
       "      <td>unitary</td>\n",
       "      <td>states</td>\n",
       "      <td>machine</td>\n",
       "      <td>gate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2          3            4  \\\n",
       "topic_0      data      matrix       model  algorithm  probability   \n",
       "topic_1       let     theorem       proof        set        space   \n",
       "topic_2      type        list        java     string         code   \n",
       "topic_3      word       words    language   sentence     document   \n",
       "topic_4  learning    training      neural   networks        layer   \n",
       "topic_5   michael  department     science   computer        image   \n",
       "topic_6     spark        data      apache    cluster       python   \n",
       "topic_7    things       world         god       mind   philosophy   \n",
       "topic_8  category         set  categories     object      objects   \n",
       "topic_9   quantum   classical       state  mechanics     learning   \n",
       "\n",
       "                   5             6          7           8            9  \n",
       "topic_0     function  distribution        log      figure        using  \n",
       "topic_1        lemma        spaces     finite  definition  proposition  \n",
       "topic_2        class      function    chapter      object      program  \n",
       "topic_3           al        corpus  sentences        text        topic  \n",
       "topic_4      network          deep      input      hidden        model  \n",
       "topic_5   dictionary        sparse      patch     patches          min  \n",
       "topic_6        model          user      https      import         docs  \n",
       "topic_7         life         human      ideas         man         time  \n",
       "topic_8  isomorphism        limits     arrows       arrow      diagram  \n",
       "topic_9          doi       unitary     states     machine         gate  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nmf_keywords_per_topic = top_topic_words(nmf, feature_names, 100)\n",
    "display(nmf_keywords_per_topic.iloc[:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:23<00:00, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN COHERENCE\n",
      "1.1402957764788306\n",
      "CPU times: user 3min 18s, sys: 3.94 s, total: 3min 22s\n",
      "Wall time: 3min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topic_coherences = [topic_coherence(keywords.values, n_top_keywords=10) for (__, keywords) in tqdm(nmf_keywords_per_topic.iterrows(), total=n_topics)]\n",
    "\n",
    "print('MEAN COHERENCE')\n",
    "print(sum(topic_coherences) / n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean      1.140296\n",
       "std       0.506678\n",
       "min       0.665035\n",
       "25%       0.835518\n",
       "50%       1.023436\n",
       "75%       1.152266\n",
       "max       2.304092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(topic_coherences).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One topic seems to contain machine learning keywords.\n",
    "Let's check what documents are on ML according to topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml/Machine learning cheat sheet.pdf', 0.3609000427699512),\n",
       " ('ml/teoria/slt/Elements of Statistical Learning.pdf', 0.3607585551283273),\n",
       " ('ml/teoria/Bayesian Reasoning and Machine Learning.pdf', 0.3466737472842171),\n",
       " ('ml/artykuły/Determinantal point processes for machine learning.pdf',\n",
       "  0.30551959916850985),\n",
       " ('dsp itd/Natural Image Statistics.pdf', 0.30156890619324384),\n",
       " ('ml/teoria/Generalized Principal Component Analysis.pdf',\n",
       "  0.28624916359073466),\n",
       " ('ml/teoria/slt/An Introduction to Statistical learning theory.pdf',\n",
       "  0.2858934308293298),\n",
       " ('ml/teoria/slt/Statistical Learning With Sparsity.pdf', 0.282509741028948),\n",
       " ('ml/teoria/algo/Randomized methods for computing low-rank approximations of matrices.pdf',\n",
       "  0.2817193620861067),\n",
       " ('ml/szeregi czasowe/Time Series Analysis and Its Applications With R Examples.pdf',\n",
       "  0.2803886556281992),\n",
       " ('dsp itd/cv/Computer vision_ A modern approach.pdf', 0.2794899468763625),\n",
       " ('ml/teoria/glrm.pdf', 0.27845025628927944),\n",
       " ('ml/sparse/Sparse_and_Redundant_Representations_From_Theory_to_Applications.pdf',\n",
       "  0.2739296893316035),\n",
       " ('dsp itd/dsp/Signals, Systems and Inference.pdf', 0.27378856666109963),\n",
       " ('ml/artykuły/Top 10 Algorithms in Data Mining.pdf', 0.2728319814666461),\n",
       " ('ml/Foundations of Data Science.pdf', 0.2703856834519864),\n",
       " ('matma zastosowania/statystyka/Statistical modelling the two cultures.pdf',\n",
       "  0.2701850502975812),\n",
       " ('ml/Mathematical Introduction to Data Science.pdf', 0.2688541666452319),\n",
       " ('ml/sieci neuronowe/Neural networks a comprehensive foundation.pdf',\n",
       "  0.26818188945738963),\n",
       " ('dsp itd/dsp/Introduction to Random Signals and Applied Kalman Filtering.pdf',\n",
       "  0.2681299273515316),\n",
       " ('ml/Data Mining and Knowledge Discovery Handbook.pdf', 0.2664670450686801),\n",
       " ('ml/ng_ml/1. Supervised Learning, Discriminative Algorithms.pdf',\n",
       "  0.26558805976808425),\n",
       " ('dsp itd/dsp/Optimum_Signal_Processing.pdf', 0.256393194425205),\n",
       " ('ml/big data/Mining Massive Datasets.pdf', 0.2542035743271838),\n",
       " ('ml/sieci neuronowe/deeplearning/ch05_ml.pdf', 0.25411853384757777)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_topic_index = 4\n",
    "get_most_representative_article_names(ml_topic_index, legal_filenames, nmf_features)[:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
