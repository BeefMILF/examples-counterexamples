{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression  in Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'hello world' in Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.73105858],\n",
       "       [ 0.26894142,  0.11920292]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is input placeholder\n",
    "x = T.dmatrix('x')\n",
    "# symbolic expression for logistic function\n",
    "s = 1 / (1 + T.exp(-x))\n",
    "# register s as a function of x\n",
    "logistic = theano.function([x], s)\n",
    "\n",
    "# call function (substitute value of x_input for x)\n",
    "x_input = [[0, 1], [-1, -2]]\n",
    "logistic(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model itself\n",
    "\n",
    "The model uses regularization with $l_1$ and $l_2$ penalty, and is trained with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "  \n",
    "  def __init__(self,\n",
    "               n_iter,\n",
    "               batch_size=1000,\n",
    "               lmbda=0.0001,\n",
    "               l1_ratio=0,\n",
    "               learning_rate=0.001,\n",
    "               random_state=0):\n",
    "    self.n_iter = n_iter\n",
    "    self.l1_ratio = l1_ratio\n",
    "    self.lmbda = lmbda\n",
    "    self.learning_rate = learning_rate \n",
    "    self.batch_size = batch_size \n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "   \n",
    "    self.is_fitted = False\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    if self.batch_size:\n",
    "      n_examples = self.batch_size\n",
    "    else:\n",
    "      n_examples = X.shape[0]\n",
    "    \n",
    "    n_dim = X.shape[1]\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    # inputs\n",
    "    self.thX = T.dmatrix('thX')\n",
    "    self.thy = T.vector('thy', dtype='int64')\n",
    "    \n",
    "    # weights\n",
    "    self.thW, self.thB = self.__initialized_weights(n_dim, n_classes)\n",
    "    \n",
    "    # calculate probability and loss \n",
    "    Z = T.dot(self.thX, self.thW) + self.thB\n",
    "    self.p_y_by_x = T.nnet.softmax(Z)\n",
    "    # negative log likelihood\n",
    "    ll = (T.log(self.p_y_by_x)\n",
    "          [T.arange(n_examples), self.thy]) \n",
    "    nll = - T.mean(ll)\n",
    "      \n",
    "    regularization = self.__regularization(self.thW, self.lmbda, self.l1_ratio)\n",
    "    \n",
    "    self.loss = nll + regularization\n",
    "    \n",
    "    updates = self.__updates(\n",
    "      self.loss,\n",
    "      self.thW,\n",
    "      self.thB,\n",
    "      self.learning_rate)\n",
    "   \n",
    "    # setup training\n",
    "    self.train_model = theano.function(\n",
    "      inputs=[self.thX, self.thy],\n",
    "      outputs=self.loss,\n",
    "      updates=updates)\n",
    "    \n",
    "    self.__iter_training(self.train_model, X, y, self.n_iter, self.batch_size)\n",
    " \n",
    "\n",
    "  def predict(self, X):\n",
    "    if self.is_fitted:\n",
    "      return self.__prediction_function()(X)\n",
    "    else:\n",
    "      raise NotFittedError\n",
    "\n",
    "      \n",
    "  def score(self, X, y):\n",
    "    y_pred = self.predict(X)\n",
    "    return accuracy_score(y, y_pred)\n",
    "      \n",
    "       \n",
    "  def __initialized_weights(self, n_dim, n_classes):\n",
    "    \"\"\"\n",
    "    initialize weights (shared variables)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize class weights\n",
    "    thW = theano.shared(\n",
    "        value=np.zeros(\n",
    "          (n_dim, n_classes),\n",
    "          dtype=theano.config.floatX),\n",
    "        name='thW',\n",
    "        borrow=True)\n",
    "\n",
    "    # initialize the biases b as a vector of n_out 0s\n",
    "    thB = theano.shared(\n",
    "      value=np.zeros(\n",
    "        (n_classes,),\n",
    "        dtype=theano.config.floatX),\n",
    "      name='thB',\n",
    "      borrow=True)\n",
    "    return thW, thB\n",
    "       \n",
    "    \n",
    "  def __prediction_function(self):\n",
    "    \"\"\"\n",
    "    actual function used for predicting y given X\n",
    "    \"\"\"\n",
    "    y_pred = T.argmax(self.p_y_by_x, axis=1)\n",
    "    return theano.function(\n",
    "      inputs=[self.thX],\n",
    "      outputs=y_pred)\n",
    "  \n",
    "  \n",
    "  def __regularization(self, W, lmbda, l1_ratio):\n",
    "    \"\"\"\n",
    "    regularization with l1 and l2 weight penalties\n",
    "    \"\"\"\n",
    "    weight_penalty = T.sum(W ** 2)\n",
    "    l1_penalty = T.sum(abs(W))\n",
    "    return  (lmbda * \n",
    "              ((1 - l1_ratio) * weight_penalty +\n",
    "               l1_ratio * l1_penalty))\n",
    "  \n",
    "  \n",
    "  def __updates(self, loss, W, B, learning_rate):\n",
    "    \"\"\"\n",
    "    gradient descent updates\n",
    "    \"\"\"\n",
    "    \n",
    "    # gradients\n",
    "    gW = T.grad(cost=loss, wrt=W)\n",
    "    gB = T.grad(cost=loss, wrt=B)\n",
    "      \n",
    "    updates = [\n",
    "      (W, W - learning_rate * gW),\n",
    "      (B, B - learning_rate * gB)]\n",
    "    return updates\n",
    "  \n",
    "  \n",
    "  def __iter_training(self, train_model, X, y, n_iter, batch_size):\n",
    "    \"\"\"\n",
    "    iterate weight updates n_iter times and store loss for each step\n",
    "    \"\"\" \n",
    "    def get_batch(batch_size):\n",
    "      if batch_size:\n",
    "        indices = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "        return X[indices, :], y[indices] \n",
    "      else:\n",
    "        return X, y\n",
    "      \n",
    "    self.losses = []\n",
    "    for __ in range(n_iter):\n",
    "      X_batch, y_batch = get_batch(batch_size)\n",
    "      current_loss = train_model(X_batch, y_batch)\n",
    "      self.losses.append(current_loss)\n",
    "    \n",
    "    self.losses = np.array(self.losses)\n",
    "  \n",
    "    self.is_fitted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X = mnist['data']\n",
    "y = mnist['target'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nnets/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sscaler = StandardScaler()\n",
    "X_train = sscaler.fit_transform(X_train)\n",
    "X_test = sscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LogisticRegression(\n",
    "  n_iter=500,\n",
    "  batch_size=None,\n",
    "  lmbda=0.001,\n",
    "  l1_ratio=0.5,\n",
    "  learning_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lreg.losses)\n",
    "plt.title('Training loss across iterations')\n",
    "plt.xlabel('No steps')\n",
    "plt.ylabel('Loss')\n",
    "print('Final loss: {:.4f}'.format(lreg.losses[-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "y_pred = lreg.predict(X_test)\n",
    "print('accuracy: {:.4f}'.format(lreg.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lreg = LogisticRegression(\n",
    "  n_iter=500,\n",
    "  batch_size=1000,\n",
    "  lmbda=0.001,\n",
    "  l1_ratio=0.5,\n",
    "  learning_rate=0.25)\n",
    "\n",
    "%time sgd_lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_width = 5\n",
    "smoothed_sgd_loss = np.convolve(\n",
    "  sgd_lreg.losses,\n",
    "  v = np.ones(ma_width) / ma_width,\n",
    "  mode='valid')\n",
    "\n",
    "plt.plot(sgd_lreg.losses)\n",
    "plt.plot(smoothed_sgd_loss)\n",
    "plt.title('Training loss (and smoothed) across iterations')\n",
    "plt.xlabel('No steps')\n",
    "plt.ylabel('Loss')\n",
    "print('Final loss: {:.4f}'.format(sgd_lreg.losses[-1]))\n",
    "print('Final loss, smoothed: {:.4f}'.format(smoothed_sgd_loss[-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd_lreg.predict(X_test)\n",
    "print('accuracy: {:.4f}'.format(sgd_lreg.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = lreg.thB.get_value()\n",
    "\n",
    "for i in range(10):\n",
    "  digit_weights = lreg.thW.get_value().T[i, :].reshape(28, 28)\n",
    "  sgd_digit_weights = sgd_lreg.thW.get_value().T[i, :].reshape(28, 28)\n",
    "  f, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n",
    "  f.suptitle('Class {}'.format(i)) \n",
    "  \n",
    "  \n",
    "  ax1.imshow(digit_weights , cmap='gray')\n",
    "  ax1.set_title('weights')\n",
    "  ax1.axis('off') \n",
    "  \n",
    "  ax2.imshow(sgd_digit_weights, cmap='gray')\n",
    "  ax2.set_title('sgd weights')\n",
    "  ax2.axis('off') \n",
    "  \n",
    "  ax3.imshow(abs(digit_weights - sgd_digit_weights), cmap='gray')\n",
    "  ax3.set_title('absolute difference'.format(str(i)))\n",
    "  ax3.axis('off') \n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
