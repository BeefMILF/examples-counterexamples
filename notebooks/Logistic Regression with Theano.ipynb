{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort of 'hello world' in Theano\n",
    "\n",
    "x = T.dmatrix('x')\n",
    "s = 1 / (1 + T.exp(-x))\n",
    "logistic = theano.function([x], s)\n",
    "\n",
    "out = logistic([[0, 1], [-1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing elasticnet logistic regression (trained with gradient descent) in Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "  \n",
    "  def __init__(self, n_iter, lmbda=0.0001, l1_ratio=0, learning_rate=0.001):\n",
    "    self.n_iter = n_iter\n",
    "    self.is_fitted = False\n",
    "    self.l1_ratio = l1_ratio\n",
    "    self.lmbda = lmbda\n",
    "    self.learning_rate = learning_rate \n",
    "   \n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    n_dim = X.shape[1]\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    self.thX = T.dmatrix('thX')\n",
    "    self.thy = T.vector('thy', dtype='int64')\n",
    "     \n",
    "    self.thW, self.thB = self.__initialized_weights(n_dim, n_classes)\n",
    "    \n",
    "    # calculate probability and loss \n",
    "    self.p_y_by_x = T.nnet.softmax(\n",
    "      T.dot(self.thX, self.thW) + self.thB)\n",
    "    # negative log likelihood\n",
    "    nll = (- T.mean(\n",
    "      T.log(self.p_y_by_x)[T.arange(y_train.shape[0]),self.thy]))\n",
    "      \n",
    "    regularization = self.__regularization(self.thW, self.lmbda, self.l1_ratio)\n",
    "    self.loss = nll + regularization\n",
    "    \n",
    "    self.y_pred = T.argmax(self.p_y_by_x, axis=1)\n",
    "    \n",
    "    # calculate gradients\n",
    "    self.gW = T.grad(cost=self.loss, wrt=self.thW)\n",
    "    self.gB = T.grad(cost=self.loss, wrt=self.thB)\n",
    "    \n",
    "    updates = [\n",
    "      (self.thW,\n",
    "       self.thW - self.learning_rate * self.gW),\n",
    "      (self.thB,\n",
    "         self.thB - self.learning_rate * self.gB)]\n",
    "    \n",
    "    # setup training\n",
    "    self.train_model = theano.function(\n",
    "      inputs=[self.thX, self.thy],\n",
    "      outputs=self.loss,\n",
    "      updates=updates\n",
    "    )\n",
    "    \n",
    "    # actual training\n",
    "    for __ in range(self.n_iter):\n",
    "      self.train_model(X, y)\n",
    "  \n",
    "    self.is_fitted = True\n",
    " \n",
    "\n",
    "  def predict(self, X):\n",
    "    if self.is_fitted:\n",
    "      return self.__prediction_function()(X)\n",
    "    else:\n",
    "      raise NotFittedError\n",
    "\n",
    "      \n",
    "  def score(self, X, y):\n",
    "    y_pred = self.predict(X)\n",
    "    return accuracy_score(y, y_pred)\n",
    "      \n",
    "       \n",
    "  def __initialized_weights(self, n_dim, n_classes):\n",
    "    thW = theano.shared(\n",
    "        value=np.zeros(\n",
    "          (n_dim, n_classes),\n",
    "          dtype=theano.config.floatX),\n",
    "        name='thW',\n",
    "        borrow=True)\n",
    "\n",
    "    # initialize the biases b as a vector of n_out 0s\n",
    "    thB = theano.shared(\n",
    "      value=np.zeros(\n",
    "        (n_classes,),\n",
    "        dtype=theano.config.floatX),\n",
    "      name='thB',\n",
    "      borrow=True)\n",
    "    return thW, thB\n",
    "       \n",
    "    \n",
    "  def __prediction_function(self):\n",
    "    return theano.function(\n",
    "      inputs=[self.thX],\n",
    "      outputs=self.y_pred)\n",
    "  \n",
    "  \n",
    "  def __regularization(self, W, lmbda, l1_ratio):\n",
    "    weight_penalty = T.sum(W ** 2)\n",
    "    l1_penalty = T.sum(abs(W))\n",
    "    return  (lmbda * \n",
    "              ((1 - l1_ratio) * weight_penalty +\n",
    "               l1_ratio * l1_penalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lreg = LogisticRegression(\n",
    "  n_iter=100,\n",
    "  lmbda=0.001,\n",
    "  l1_ratio=0,\n",
    "  learning_rate=0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 548 ms, sys: 36 ms, total: 584 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%time lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        54\n",
      "          1       0.87      0.96      0.91        55\n",
      "          2       1.00      0.98      0.99        53\n",
      "          3       0.94      0.93      0.94        55\n",
      "          4       1.00      0.96      0.98        54\n",
      "          5       0.93      0.96      0.95        55\n",
      "          6       1.00      0.98      0.99        54\n",
      "          7       0.95      1.00      0.97        54\n",
      "          8       0.92      0.85      0.88        52\n",
      "          9       1.00      0.96      0.98        54\n",
      "\n",
      "avg / total       0.96      0.96      0.96       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "y_pred = lreg.predict(X_test)\n",
    "print('accuracy: {:0.4f}'.format(lreg.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
