{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximate Nearest Neighbors for search\n",
    "\n",
    "* Author\n",
    "* some terminology\n",
    "* kNN\n",
    "* Applications\n",
    "    - information retrieval\n",
    "    - useful in more complex tasks (pattern matching)\n",
    "* Why ANN\n",
    "* Approaches\n",
    "    - LSH\n",
    "    - Dimensionality-reduction based methods\n",
    "    - Graph-based\n",
    "    - Product Quantization\n",
    "* Libraries\n",
    "    - annoy\n",
    "    - NMSLib\n",
    "    - Rii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Author\n",
    "\n",
    "#### Jakub Bartczuk\n",
    "\n",
    "- Machine learning engineer, previously developer\n",
    "- Currently: Data scientist @Semantive\n",
    "- Math background (Theoretical Math BSc)\n",
    "\n",
    "For a previous year heavily into deep learning. Actually I started using ANN tools in a DL-based project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN Terminology\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/279px-KnnClassification.svg.png\">\n",
    "\n",
    "Let's fix some notation:\n",
    "\n",
    "$X$ - dataset, $X \\subset \\mathbb{R}^d$\n",
    "\n",
    "$y$ - target in supervised learning\n",
    "\n",
    "$\\hat{y}$ - estimate (output of model) in supervised learning\n",
    "\n",
    "\n",
    "$\\| x - y \\|_{p} = \\sqrt[p]{\\sum_{i<d} (x_i - y_i)^p}$ - $p$th norm, for example $ \\| x - y \\|_{2} $ - Euclidean norm\n",
    "\n",
    "$q$ - query vector\n",
    "\n",
    "$N_k(q, X)$ - $k$ nearest neighbors of $q$ in $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN\n",
    "\n",
    "Probably one of the simplest ML methods.\n",
    "\n",
    "Pros\n",
    "- doesn't make any assumption on distribution in supervised learning (nonparametric)\n",
    "- quite easy to interpret prediction - just look at the neighbors!\n",
    "\n",
    "Cons\n",
    "- prone to overfitting\n",
    "- suffers from 'curse of dimensionality'\n",
    "- **costly inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Information retrieval\n",
    "\n",
    "Traditionally IR is mostly done for text data that is easily searchable in a different way (inverted index).\n",
    "\n",
    "kNN enables IR of any data that we can vectorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Examples:\n",
    "- music (there are many tools for extracting features from sound)\n",
    "- images (just run your favourite CNN and extract activations from some modestly sized layer)\n",
    "- text again - extract features using Doc2Vec or some RNN of your choice\n",
    "- actually anything for which you have a DL model that captures relevant structure\n",
    "\n",
    "A concrete example of music search can be found in [findkit](https://github.com/lambdaofgod/findkit) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Useful in other ML task\n",
    "\n",
    "- kNN is used as intermediate step in manifold learning algorithms (picture from megaman documentation) <img src=\"http://mmp2.github.io/megaman/_images/spectra_Halpha.png\" style=\"float: right;\" width=\"350\"/>\n",
    "\n",
    "- Texture synthesis - for example see [Style-Transfer via Texture-Synthesis](https://arxiv.org/pdf/1609.03057.pdf) (that's a non-CNN based Style Transfer method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN for search\n",
    "\n",
    "### suffers from 'curse of dimensionality'\n",
    "\n",
    "Why care? Most DL model layers don't have more than a couple of hundreds-dimensional outputs.\n",
    "\n",
    "### costly inference\n",
    "\n",
    "That's actually the biggest problem. \n",
    "\n",
    "TODO: O(n) complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximate kNN\n",
    "\n",
    "Idea: don't do *exact* kNN. Try to retrieve neighbors *with high probability*.\n",
    "\n",
    "### Approaches\n",
    "- LSH\n",
    "- Dimensionality-reduction based methods\n",
    "- Graph-based\n",
    "- Product Quantization"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nnets",
   "language": "python",
   "name": "nnets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
